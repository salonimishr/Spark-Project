{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALIDATION\n",
    "\n",
    "For validation of topic modeling best is to do coherence score as this measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic but Spark doesn't have any package for that therefore perplexity measure is used here for validation. \n",
    "\n",
    "Perplexity is a statistical measure of how well a probability model predicts a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#System.setProperty(\"hadoop.home.dir\", \"E:\\software\\spark-3.0.0-preview2-bin-hadoop2.7\\spark-3.0.0-preview2-bin-hadoop2.7\\winutils.exe\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "EXE_MEMORY = \"2g\"\n",
    "DRIVER_MEMORY = \"8g\"\n",
    "spark = SparkSession.builder.appName(\"AWSNLP\").config(\"spark.executor.memory\", EXE_MEMORY).config(\"spark.executor.cores\", \"2\").config(\"spark.driver.memory\", DRIVER_MEMORY).config(\"spark.cores.max\", \"10\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('C:\\\\Users\\\\salon\\\\Downloads\\\\Magazine_Subscriptions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.select('reviewText', \"overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "cv=CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "\n",
    "model=cv.fit(df_final_words)\n",
    "\n",
    "result=model.transform(df_final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training,test)=result.randomSplit([0.7,0.3], seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62765\n",
      "26891\n"
     ]
    }
   ],
   "source": [
    "print(str(training.count()))\n",
    "print(str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+--------------------+\n",
      "|IDX|overall|               words|            features|\n",
      "+---+-------+--------------------+--------------------+\n",
      "|  0|    5.0|[comput, enthusia...|(44681,[4,5,7,8,1...|\n",
      "|  1|    5.0|[thank, ziff, dav...|(44681,[9,10,48,4...|\n",
      "|  2|    3.0|[antiqu, magazin,...|(44681,[0,1,3,5,6...|\n",
      "|  4|    5.0|[great, read, eve...|(44681,[1,2,4,25]...|\n",
      "|  5|    3.0|[magazin, great, ...|(44681,[0,2,9,10,...|\n",
      "+---+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "#lda=LDA().fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=20\n",
    "max_iterations=500\n",
    "lda_model = LDA(k=num_topics, maxIter=max_iterations)\n",
    "modelda=lda_model.fit(training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERPLEXITY \n",
    "\n",
    "From LDA model, we got the value of logPerplexity in training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp=modelda.logPerplexity(test),modelda.logPerplexity(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lp=lda.logPerplexity(test),modelda.logPerplexity(training)  # different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7.695705426903245, 7.227145075067786)\n"
     ]
    }
   ],
   "source": [
    "print(lp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
