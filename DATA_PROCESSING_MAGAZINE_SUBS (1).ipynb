{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#System.setProperty(\"hadoop.home.dir\", \"E:\\software\\spark-3.0.0-preview2-bin-hadoop2.7\\spark-3.0.0-preview2-bin-hadoop2.7\\winutils.exe\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "EXE_MEMORY = \"2g\"\n",
    "DRIVER_MEMORY = \"8g\"\n",
    "spark = SparkSession.builder.appName(\"AWSNLP\").config(\"spark.executor.memory\", EXE_MEMORY).config(\"spark.executor.cores\", \"2\").config(\"spark.driver.memory\", DRIVER_MEMORY).config(\"spark.cores.max\", \"10\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json('C:\\\\Users\\\\salon\\\\Downloads\\\\Magazine_Subscriptions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.select('reviewText', \"overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---+\n",
      "|          reviewText|overall|idx|\n",
      "+--------------------+-------+---+\n",
      "|for computer enth...|    5.0|  1|\n",
      "|Thank god this is...|    5.0|  2|\n",
      "+--------------------+-------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.sql import functions as F\n",
    "df = df.withColumn(\"idx\", F.monotonically_increasing_id())\n",
    "windowSpec = W.orderBy(\"idx\")\n",
    "df.withColumn(\"idx\", F.row_number().over(windowSpec)).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PUNCTUATION REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
    "def removePunctuation(column):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Note:\n",
    "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        column (Column): A Column containing a sentence.\n",
    "\n",
    "    Returns:\n",
    "        Column: A Column named 'sentence' with clean-up operations applied.\n",
    "    \"\"\"\n",
    "    return trim(lower(regexp_replace(column, '[^\\sa-zA-Z0-9]', ''))).alias('reviewText')\n",
    "\n",
    "df=df.select(\"IDX\", \"overall\", (removePunctuation(col('reviewText'))))\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVING NULL VALUES\n",
    "\n",
    "Null values removed from dataset where reviewText had no texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.where(df.reviewText.isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PROCESSING\n",
    "\n",
    "Dataset processed here by tokenizing, removing stop words(list of stopwords removed is given at last), lemmatization, stemming and removing words that were of length 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.types import *\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol='reviewText', outputCol='words_token')\n",
    "df_words_token = tokenizer.transform(df).select('IDX',\"overall\", 'words_token')\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol='words_token', outputCol='words_clean')\n",
    "df_words_no_stopw = remover.transform(df_words_token).select('IDX',\"overall\", 'words_clean')\n",
    "\n",
    "#lemmatization\n",
    "lemm=WordNetLemmatizer()\n",
    "lemm_udf=udf(lambda tokens:[lemm.lemmatize(token) for token in tokens], ArrayType(StringType()))\n",
    "df_lemm = df_words_no_stopw.withColumn(\"lemmi\", lemm_udf(\"words_clean\")).select('IDX',\"overall\", 'lemmi')\n",
    "\n",
    "# Stem text\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "df_stemmed = df_lemm.withColumn(\"words_stemmed\", stemmer_udf(\"lemmi\")).select('IDX',\"overall\",'words_stemmed')\n",
    "\n",
    "\n",
    "# Filter length word > 3\n",
    "filter_length_udf = udf(lambda row: [x for x in row if len(x) > 3], ArrayType(StringType()))\n",
    "df_final_words = df_stemmed.withColumn('words', filter_length_udf(col('words_stemmed'))).select('IDX',\"overall\", 'words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the dataframes after performing subsequent actions.\n",
    "\n",
    "Here are only showing 5 values of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "|IDX|overall|          reviewText|\n",
      "+---+-------+--------------------+\n",
      "|  0|    5.0|for computer enth...|\n",
      "|  1|    5.0|thank god this is...|\n",
      "|  2|    3.0|antiques magazine...|\n",
      "|  3|    5.0|this beautiful ma...|\n",
      "|  4|    5.0|a great read ever...|\n",
      "+---+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+-------+--------------------+\n",
      "|IDX|overall|         words_token|\n",
      "+---+-------+--------------------+\n",
      "|  0|    5.0|[for, computer, e...|\n",
      "|  1|    5.0|[thank, god, this...|\n",
      "|  2|    3.0|[antiques, magazi...|\n",
      "|  3|    5.0|[this, beautiful,...|\n",
      "|  4|    5.0|[a, great, read, ...|\n",
      "+---+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+-------+--------------------+\n",
      "|IDX|overall|         words_clean|\n",
      "+---+-------+--------------------+\n",
      "|  0|    5.0|[computer, enthus...|\n",
      "|  1|    5.0|[thank, god, ziff...|\n",
      "|  2|    3.0|[antiques, magazi...|\n",
      "|  3|    5.0|[beautiful, magaz...|\n",
      "|  4|    5.0|[great, read, eve...|\n",
      "+---+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+-------+--------------------+\n",
      "|IDX|overall|               lemmi|\n",
      "+---+-------+--------------------+\n",
      "|  0|    5.0|[computer, enthus...|\n",
      "|  1|    5.0|[thank, god, ziff...|\n",
      "|  2|    3.0|[antique, magazin...|\n",
      "|  3|    5.0|[beautiful, magaz...|\n",
      "|  4|    5.0|[great, read, eve...|\n",
      "+---+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+-------+--------------------+\n",
      "|IDX|overall|       words_stemmed|\n",
      "+---+-------+--------------------+\n",
      "|  0|    5.0|[comput, enthusia...|\n",
      "|  1|    5.0|[thank, god, ziff...|\n",
      "|  2|    3.0|[antiqu, magazin,...|\n",
      "|  3|    5.0|[beauti, magazin,...|\n",
      "|  4|    5.0|[great, read, eve...|\n",
      "+---+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+-------+--------------------+\n",
      "|IDX|overall|               words|\n",
      "+---+-------+--------------------+\n",
      "|  0|    5.0|[comput, enthusia...|\n",
      "|  1|    5.0|[thank, ziff, dav...|\n",
      "|  2|    3.0|[antiqu, magazin,...|\n",
      "|  3|    5.0|[beauti, magazin,...|\n",
      "|  4|    5.0|[great, read, eve...|\n",
      "+---+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)\n",
    "df_words_token.show(5)\n",
    "df_words_no_stopw.show(5)\n",
    "df_lemm.show(5)\n",
    "df_stemmed.show(5)\n",
    "df_final_words.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', \"i'll\", \"you'll\", \"he'll\", \"she'll\", \"we'll\", \"they'll\", \"i'd\", \"you'd\", \"he'd\", \"she'd\", \"we'd\", \"they'd\", \"i'm\", \"you're\", \"he's\", \"she's\", \"it's\", \"we're\", \"they're\", \"i've\", \"we've\", \"you've\", \"they've\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\", \"don't\", \"doesn't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"mustn't\", \"can't\", \"couldn't\", 'cannot', 'could', \"here's\", \"how's\", \"let's\", 'ought', \"that's\", \"there's\", \"what's\", \"when's\", \"where's\", \"who's\", \"why's\", 'would']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "# Define a list of stop words or use default \n",
    "remover = StopWordsRemover()\n",
    "stopwords = remover.getStopWords() # Display default \n",
    "print(stopwords[:200])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
